{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Text](images.jfif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:40.751508Z",
     "iopub.status.busy": "2021-07-02T02:53:40.751364Z",
     "iopub.status.idle": "2021-07-02T02:53:41.867164Z",
     "shell.execute_reply": "2021-07-02T02:53:41.866577Z",
     "shell.execute_reply.started": "2021-07-02T02:53:40.751492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wget in ./.local/lib/python3.7/site-packages (3.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/opt/tljh/user/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:41.868347Z",
     "iopub.status.busy": "2021-07-02T02:53:41.868170Z",
     "iopub.status.idle": "2021-07-02T02:53:43.339406Z",
     "shell.execute_reply": "2021-07-02T02:53:43.338835Z",
     "shell.execute_reply.started": "2021-07-02T02:53:41.868326Z"
    }
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import string\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.340403Z",
     "iopub.status.busy": "2021-07-02T02:53:43.340234Z",
     "iopub.status.idle": "2021-07-02T02:53:43.395287Z",
     "shell.execute_reply": "2021-07-02T02:53:43.394737Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.340384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“ Good afternoon, and thank you and, wow. I am so privileged and so honored to be sharing this afternoon with all of you and these incredibly amazing women that are being honored today.\n",
      "I’d like to extend my congratulations to each one of you, Octavia, Michelle, Kelly, Patty, and all fifty women that have been included in the impact report.\n",
      "Your achievements not just inspire me but also so many others to work harder to be better and to make a dent wherever we can.\n",
      "So, I’m very, very proud to be standing alongside you.\n",
      "So, in life you know there are moments when you stop and ask yourself: “How did I get here?”\n",
      "Like: “Why am I standing here?”\n",
      "Well, this is definitely one of those moments for me and I find myself going back to the beginning, back to my roots.\n",
      "I was born to incredible parents, amazing parents who served as doctors in the Indian Army.\n",
      "I was the first born and as far back as I can remember I made my parents very proud and happy 99% of the time.\n",
      "Okay, slight exaggerations of personal achievements are allowed from time to time, don’t you think?\n",
      "My brother was born a few years later and even then, nothing changed for me.\n",
      "We were both given equal opportunities, and I want to emphasize this, I want to really emphasize this for you because I don’t think a lot of people might understand that being equal might seem very normal but where I come from India and a lot of developing countries around the world more of not this is an exception.\n",
      "It’s actually a privilege.\n",
      "My first experience of the glaring disparity between boys and girls came at a very, very young age.\n",
      "I grew up in a middle-class family with extremely philanthropic parents who constantly reminded me and my brother how lucky we were and how giving back to those who were less fortunate was not a choice it was a way of life.\n",
      "Simple.\n",
      "I was seven or eight years old when my parents started taking me on these visits in a traveling clinic to developing communities around and villages around the city that we lived in called Bareilly.\n",
      "We were packed into this ambulance and would my parents would provide free medical care to people who couldn’t afford it.\n",
      "My job at the age of eight was an assistant pharmacist.\n",
      "I would count all the medicines put them in an envelope and give it out to patients, and I really took my job very seriously, very seriously.\n",
      "But the more I went on these expeditions, the more I began to notice the simplest things that distinguished a boy from a girl or a man from a woman.\n",
      "For example, girls were pulled out of school when they hit puberty because they were considered ready for marriage and babies.\n",
      "That’s 12 and 13 while boys still enjoyed their childhood.\n",
      "Or basic human rights such as health care were denied just because they were women.\n",
      "Let this, let’s call this whole experience trigger number one for me.\n",
      "Fast-forward a few years and many, many triggers in between.\n",
      "Like a producer-director for example early on in my career, I must have been about 18 or 19, telling me that if I didn’t agree to the ridiculous terms or painfully low salary in his movie that he would just replace me because girls are replaceable in the entertainment business.\n",
      "That was a memorable one.\n",
      "Made me decide to make myself irreplaceable.\n",
      "But I think what really moved the needle for me and ultimately led me to create the Priyanka Chopra foundation for health and education and around the same time partner with UNICEF was an encounter with my housekeeper’s daughter.\n",
      "About 12 years ago I came home from set early one day and she was sitting in my library reading a book and she must have been eight or nine years old and I knew she loved reading.\n",
      "So, I asked her, I was like, this is, I mean, it’s a weekday why aren’t you in school?\n",
      "And she said: “Oh, I don’t go to school anymore.”\n",
      "So, I went and asked her mother and I said, you know: “Why isn’t she in school?”\n",
      "And her mom said that her family couldn’t afford to send her and her brother’s to school, so they chose the boys.\n",
      "The reason, she would eventually get married and it would be a waste of money.\n",
      "I was completely blown, and it shook me to my core.\n",
      "Eventually, I decided to cover the cost of her education so that she could continue to learn because education is a basic human right.\n",
      "And a huge necessity especially today.\n",
      "From that point on I was determined to make a difference and as many children’s lives as I could.\n",
      "In whatever big or small way that I could contribute.\n",
      "There’s a really, really beautiful quote that I read recently, and I think it’s absolutely appropriate to say, to explain what I’m trying to say today.\n",
      "“The hand that rocks the cradle, the procreator, the mother of tomorrow; a woman shapes the destiny of civilization. Such is the tragic irony of fate, that a beautiful creation such as a girl child is today one of the gravest concerns facing humanity.”\n",
      "Girls have the power to change the world.\n",
      "It is a fact and yet today girls are more likely than boys never to set foot in a classroom.\n",
      "Despite all the efforts and progress made over the last two decades. More than, I’m just gonna give you a stat, more than 15 million girls of primary school age will never learn how to read or write compared to 10 million boys.\n",
      "Primary school it’s the beginning of our future.\n",
      "Over the last 11 years, I have witnessed firsthand the incredible work that UNICEF does for children around the world. Especially victims and survivors of child marriage, displacement, war, sexual violence.\n",
      "But there is still so much work to do.\n",
      "And for me, that is the fuel to my fire.\n",
      "The reason I’m so committed to this cause and that is where my passion stems from because I know that a girl’s education not just empowers families but communities and economies.\n",
      "A result of her education we all do better. It’s just as simple as that.\n",
      "As entertainers and influencers sitting in this room I feel that is our social responsibility to be a voice for the voiceless, which is why I applaud each and every woman in this room for being such a badass.\n",
      "For using your platform and your voice to contribute to change and for ensuring that there is not even one lost generation as long as we are alive.\n",
      "I’d like to thank variety and all of you for encouraging me and all of us in this room to keep going and fighting on.\n",
      "Thank you so much.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://cainvas-static.s3.amazonaws.com/media/user_data/Bhavyaaa/Speech.txt'\n",
    "Speech = wget.download(url)\n",
    "response = open(Speech,'r')\n",
    "response = response.read()\n",
    "print(response)\n",
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.396133Z",
     "iopub.status.busy": "2021-07-02T02:53:43.395978Z",
     "iopub.status.idle": "2021-07-02T02:53:43.400812Z",
     "shell.execute_reply": "2021-07-02T02:53:43.400383Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.396116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“ Good afternoon, and thank you and, wow. I am so privileged and so honored to be sharing this afternoon with all of you and these incredibly amazing women that are being honored today.',\n",
       " 'I’d like to extend my congratulations to each one of you, Octavia, Michelle, Kelly, Patty, and all fifty women that have been included in the impact report.',\n",
       " 'Your achievements not just inspire me but also so many others to work harder to be better and to make a dent wherever we can.',\n",
       " 'So, I’m very, very proud to be standing alongside you.',\n",
       " 'So, in life you know there are moments when you stop and ask yourself: “How did I get here?”',\n",
       " 'Like: “Why am I standing here?”']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = response.split(\"\\n\")\n",
    "data[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.402701Z",
     "iopub.status.busy": "2021-07-02T02:53:43.402490Z",
     "iopub.status.idle": "2021-07-02T02:53:43.405477Z",
     "shell.execute_reply": "2021-07-02T02:53:43.404986Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.402683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 56\n"
     ]
    }
   ],
   "source": [
    "print(\"Total lines:\",len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.406680Z",
     "iopub.status.busy": "2021-07-02T02:53:43.406529Z",
     "iopub.status.idle": "2021-07-02T02:53:43.410017Z",
     "shell.execute_reply": "2021-07-02T02:53:43.409579Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.406664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“ Good afternoon, an'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \" \".join(data)\n",
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.410757Z",
     "iopub.status.busy": "2021-07-02T02:53:43.410617Z",
     "iopub.status.idle": "2021-07-02T02:53:43.414014Z",
     "shell.execute_reply": "2021-07-02T02:53:43.413568Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.410741Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans(\"\",\"\",string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.414727Z",
     "iopub.status.busy": "2021-07-02T02:53:43.414589Z",
     "iopub.status.idle": "2021-07-02T02:53:43.418812Z",
     "shell.execute_reply": "2021-07-02T02:53:43.418379Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.414711Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = clean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.419975Z",
     "iopub.status.busy": "2021-07-02T02:53:43.419731Z",
     "iopub.status.idle": "2021-07-02T02:53:43.426152Z",
     "shell.execute_reply": "2021-07-02T02:53:43.425555Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.419947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1067\n",
      "['good afternoon and thank you and wow i am so privileged and so honored to be sharing this afternoon with all of you and these incredibly amazing women that are being honored today like to extend my congratulations to each one of you octavia michelle kelly patty and all fifty women', 'afternoon and thank you and wow i am so privileged and so honored to be sharing this afternoon with all of you and these incredibly amazing women that are being honored today like to extend my congratulations to each one of you octavia michelle kelly patty and all fifty women that', 'and thank you and wow i am so privileged and so honored to be sharing this afternoon with all of you and these incredibly amazing women that are being honored today like to extend my congratulations to each one of you octavia michelle kelly patty and all fifty women that have', 'thank you and wow i am so privileged and so honored to be sharing this afternoon with all of you and these incredibly amazing women that are being honored today like to extend my congratulations to each one of you octavia michelle kelly patty and all fifty women that have been', 'you and wow i am so privileged and so honored to be sharing this afternoon with all of you and these incredibly amazing women that are being honored today like to extend my congratulations to each one of you octavia michelle kelly patty and all fifty women that have been included', 'and wow i am so privileged and so honored to be sharing this afternoon with all of you and these incredibly amazing women that are being honored today like to extend my congratulations to each one of you octavia michelle kelly patty and all fifty women that have been included in']\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "lines = []\n",
    "for i in range(length,len(tokens)):\n",
    "    sequence = tokens[i-length:i]\n",
    "    line = \" \".join(sequence)\n",
    "    lines.append(line)\n",
    "print(len(lines)) \n",
    "print(lines[:6])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.427207Z",
     "iopub.status.busy": "2021-07-02T02:53:43.426978Z",
     "iopub.status.idle": "2021-07-02T02:53:43.430953Z",
     "shell.execute_reply": "2021-07-02T02:53:43.430362Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.427178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line: good afternoon and thank you and wow i am so privileged and so honored to be sharing this afternoon with all of you and these incredibly amazing women that are being honored today like to extend my congratulations to each one of you octavia michelle kelly patty and all fifty women\n",
      "First token: good\n"
     ]
    }
   ],
   "source": [
    "print(\"First line:\",lines[0])\n",
    "print(\"First token:\",tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.432118Z",
     "iopub.status.busy": "2021-07-02T02:53:43.431883Z",
     "iopub.status.idle": "2021-07-02T02:53:43.435683Z",
     "shell.execute_reply": "2021-07-02T02:53:43.435087Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.432091Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.436842Z",
     "iopub.status.busy": "2021-07-02T02:53:43.436613Z",
     "iopub.status.idle": "2021-07-02T02:53:43.498654Z",
     "shell.execute_reply": "2021-07-02T02:53:43.498221Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.436815Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.499804Z",
     "iopub.status.busy": "2021-07-02T02:53:43.499574Z",
     "iopub.status.idle": "2021-07-02T02:53:43.502827Z",
     "shell.execute_reply": "2021-07-02T02:53:43.502218Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.499777Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = sequences[:,:-1],sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.504002Z",
     "iopub.status.busy": "2021-07-02T02:53:43.503772Z",
     "iopub.status.idle": "2021-07-02T02:53:43.507838Z",
     "shell.execute_reply": "2021-07-02T02:53:43.507221Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.503976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[434 426   1 421  16   1 432   3 156  17 430   1  17 412   2  45 428  14\n",
      " 426  56  37   6  16   1  76 424 155  75   7  36  74 412  44  46   2 418\n",
      "   9 417   2 153  25   6  16 415 414 413 411   1  37 410]\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.509000Z",
     "iopub.status.busy": "2021-07-02T02:53:43.508750Z",
     "iopub.status.idle": "2021-07-02T02:53:43.511815Z",
     "shell.execute_reply": "2021-07-02T02:53:43.511332Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.508973Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size  = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.512647Z",
     "iopub.status.busy": "2021-07-02T02:53:43.512495Z",
     "iopub.status.idle": "2021-07-02T02:53:43.516759Z",
     "shell.execute_reply": "2021-07-02T02:53:43.516326Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.512631Z"
    }
   },
   "outputs": [],
   "source": [
    "y = to_categorical(y,num_classes = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:43.517578Z",
     "iopub.status.busy": "2021-07-02T02:53:43.517427Z",
     "iopub.status.idle": "2021-07-02T02:53:45.084110Z",
     "shell.execute_reply": "2021-07-02T02:53:45.083583Z",
     "shell.execute_reply.started": "2021-07-02T02:53:43.517561Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,50,input_length = 50))\n",
    "model.add(LSTM(100, return_sequences = True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100,activation = \"relu\"))\n",
    "model.add(Dense(vocab_size,activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:45.085095Z",
     "iopub.status.busy": "2021-07-02T02:53:45.084919Z",
     "iopub.status.idle": "2021-07-02T02:53:45.090460Z",
     "shell.execute_reply": "2021-07-02T02:53:45.089976Z",
     "shell.execute_reply.started": "2021-07-02T02:53:45.085076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            21750     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 435)               43935     \n",
      "=================================================================\n",
      "Total params: 216,585\n",
      "Trainable params: 216,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:45.091315Z",
     "iopub.status.busy": "2021-07-02T02:53:45.091133Z",
     "iopub.status.idle": "2021-07-02T02:53:45.100690Z",
     "shell.execute_reply": "2021-07-02T02:53:45.100239Z",
     "shell.execute_reply.started": "2021-07-02T02:53:45.091296Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer =\"adam\" , loss =\"categorical_crossentropy\"  ,metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:53:45.101580Z",
     "iopub.status.busy": "2021-07-02T02:53:45.101427Z",
     "iopub.status.idle": "2021-07-02T02:54:18.855771Z",
     "shell.execute_reply": "2021-07-02T02:54:18.855264Z",
     "shell.execute_reply.started": "2021-07-02T02:53:45.101563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 5.8935 - accuracy: 0.0403\n",
      "Epoch 2/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 5.5077 - accuracy: 0.0356\n",
      "Epoch 3/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 5.4257 - accuracy: 0.0337\n",
      "Epoch 4/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 5.4018 - accuracy: 0.0272\n",
      "Epoch 5/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 5.3946 - accuracy: 0.0300\n",
      "Epoch 6/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 5.3856 - accuracy: 0.0422\n",
      "Epoch 7/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 5.3813 - accuracy: 0.0440\n",
      "Epoch 8/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 5.3606 - accuracy: 0.0440\n",
      "Epoch 9/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 5.2871 - accuracy: 0.0440\n",
      "Epoch 10/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 5.1661 - accuracy: 0.0384\n",
      "Epoch 11/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 5.0787 - accuracy: 0.0412\n",
      "Epoch 12/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 5.0374 - accuracy: 0.0347\n",
      "Epoch 13/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 4.9742 - accuracy: 0.0394\n",
      "Epoch 14/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 4.8886 - accuracy: 0.0450\n",
      "Epoch 15/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 4.8056 - accuracy: 0.0609\n",
      "Epoch 16/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 4.7236 - accuracy: 0.0647\n",
      "Epoch 17/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 4.6288 - accuracy: 0.0600\n",
      "Epoch 18/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 4.5370 - accuracy: 0.0731\n",
      "Epoch 19/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 4.4309 - accuracy: 0.0703\n",
      "Epoch 20/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 4.3400 - accuracy: 0.0694\n",
      "Epoch 21/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 4.2297 - accuracy: 0.0750\n",
      "Epoch 22/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 4.1413 - accuracy: 0.0797\n",
      "Epoch 23/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 4.0612 - accuracy: 0.0872\n",
      "Epoch 24/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.9634 - accuracy: 0.0947\n",
      "Epoch 25/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.9000 - accuracy: 0.0956\n",
      "Epoch 26/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.8277 - accuracy: 0.1068\n",
      "Epoch 27/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.7469 - accuracy: 0.0947\n",
      "Epoch 28/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.6409 - accuracy: 0.1162\n",
      "Epoch 29/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.5830 - accuracy: 0.1040\n",
      "Epoch 30/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.4917 - accuracy: 0.1181\n",
      "Epoch 31/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.4309 - accuracy: 0.1303\n",
      "Epoch 32/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.3225 - accuracy: 0.1443\n",
      "Epoch 33/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.2647 - accuracy: 0.1462\n",
      "Epoch 34/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.1956 - accuracy: 0.1425\n",
      "Epoch 35/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.1571 - accuracy: 0.1462\n",
      "Epoch 36/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3.0565 - accuracy: 0.1649\n",
      "Epoch 37/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.9996 - accuracy: 0.1884\n",
      "Epoch 38/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.9466 - accuracy: 0.1799\n",
      "Epoch 39/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.8936 - accuracy: 0.1949\n",
      "Epoch 40/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.8541 - accuracy: 0.2006\n",
      "Epoch 41/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.7857 - accuracy: 0.2259\n",
      "Epoch 42/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.7312 - accuracy: 0.2446\n",
      "Epoch 43/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.7052 - accuracy: 0.2437\n",
      "Epoch 44/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.6293 - accuracy: 0.2690\n",
      "Epoch 45/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.5958 - accuracy: 0.2671\n",
      "Epoch 46/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.5363 - accuracy: 0.2849\n",
      "Epoch 47/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.4836 - accuracy: 0.3261\n",
      "Epoch 48/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.4338 - accuracy: 0.3308\n",
      "Epoch 49/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.3857 - accuracy: 0.3449\n",
      "Epoch 50/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.3367 - accuracy: 0.3561\n",
      "Epoch 51/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.3231 - accuracy: 0.3402\n",
      "Epoch 52/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.2504 - accuracy: 0.3843\n",
      "Epoch 53/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.1874 - accuracy: 0.3983\n",
      "Epoch 54/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.1273 - accuracy: 0.4171\n",
      "Epoch 55/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.1044 - accuracy: 0.4255\n",
      "Epoch 56/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.0584 - accuracy: 0.4114\n",
      "Epoch 57/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2.0095 - accuracy: 0.4583\n",
      "Epoch 58/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.9429 - accuracy: 0.4789\n",
      "Epoch 59/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.9046 - accuracy: 0.4705\n",
      "Epoch 60/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.8728 - accuracy: 0.4958\n",
      "Epoch 61/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.8369 - accuracy: 0.4752\n",
      "Epoch 62/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.7834 - accuracy: 0.5080\n",
      "Epoch 63/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.7272 - accuracy: 0.5164\n",
      "Epoch 64/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.6937 - accuracy: 0.5351\n",
      "Epoch 65/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.6553 - accuracy: 0.5483\n",
      "Epoch 66/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.6005 - accuracy: 0.5708\n",
      "Epoch 67/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.5783 - accuracy: 0.5708\n",
      "Epoch 68/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.5278 - accuracy: 0.5895\n",
      "Epoch 69/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.4860 - accuracy: 0.6017\n",
      "Epoch 70/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.4462 - accuracy: 0.6101\n",
      "Epoch 71/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.4065 - accuracy: 0.6223\n",
      "Epoch 72/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.3577 - accuracy: 0.6345\n",
      "Epoch 73/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.3188 - accuracy: 0.6336\n",
      "Epoch 74/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.3111 - accuracy: 0.6439\n",
      "Epoch 75/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.2451 - accuracy: 0.6776\n",
      "Epoch 76/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.2102 - accuracy: 0.6851\n",
      "Epoch 77/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.1818 - accuracy: 0.6992\n",
      "Epoch 78/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.1574 - accuracy: 0.6973\n",
      "Epoch 79/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.1118 - accuracy: 0.7104\n",
      "Epoch 80/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.0847 - accuracy: 0.7179\n",
      "Epoch 81/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.0411 - accuracy: 0.7395\n",
      "Epoch 82/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.9993 - accuracy: 0.7479\n",
      "Epoch 83/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.9636 - accuracy: 0.7554\n",
      "Epoch 84/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.9496 - accuracy: 0.7591\n",
      "Epoch 85/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.9250 - accuracy: 0.7535\n",
      "Epoch 86/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.8904 - accuracy: 0.7807\n",
      "Epoch 87/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.8591 - accuracy: 0.7957\n",
      "Epoch 88/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.8287 - accuracy: 0.7966\n",
      "Epoch 89/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.8156 - accuracy: 0.8004\n",
      "Epoch 90/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.7848 - accuracy: 0.8126\n",
      "Epoch 91/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.7673 - accuracy: 0.8154\n",
      "Epoch 92/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.7595 - accuracy: 0.8210\n",
      "Epoch 93/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.7181 - accuracy: 0.8276\n",
      "Epoch 94/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.6846 - accuracy: 0.8529\n",
      "Epoch 95/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.6785 - accuracy: 0.8435\n",
      "Epoch 96/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.6447 - accuracy: 0.8425\n",
      "Epoch 97/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.6241 - accuracy: 0.8575\n",
      "Epoch 98/150\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.6126 - accuracy: 0.8613\n",
      "Epoch 99/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5914 - accuracy: 0.8725\n",
      "Epoch 100/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5803 - accuracy: 0.8660\n",
      "Epoch 101/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5515 - accuracy: 0.8791\n",
      "Epoch 102/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5226 - accuracy: 0.8978\n",
      "Epoch 103/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5180 - accuracy: 0.8960\n",
      "Epoch 104/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5065 - accuracy: 0.8885\n",
      "Epoch 105/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.4931 - accuracy: 0.8960\n",
      "Epoch 106/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.4764 - accuracy: 0.9035\n",
      "Epoch 107/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.4914 - accuracy: 0.9007\n",
      "Epoch 108/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.4727 - accuracy: 0.9016\n",
      "Epoch 109/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.4405 - accuracy: 0.9063\n",
      "Epoch 110/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.4110 - accuracy: 0.9288\n",
      "Epoch 111/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3919 - accuracy: 0.9325\n",
      "Epoch 112/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3795 - accuracy: 0.9344\n",
      "Epoch 113/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3646 - accuracy: 0.9428\n",
      "Epoch 114/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3550 - accuracy: 0.9325\n",
      "Epoch 115/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3456 - accuracy: 0.9391\n",
      "Epoch 116/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3346 - accuracy: 0.9381\n",
      "Epoch 117/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3366 - accuracy: 0.9438\n",
      "Epoch 118/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3264 - accuracy: 0.9428\n",
      "Epoch 119/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2936 - accuracy: 0.9588\n",
      "Epoch 120/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2938 - accuracy: 0.9522\n",
      "Epoch 121/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2811 - accuracy: 0.9569\n",
      "Epoch 122/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2678 - accuracy: 0.9616\n",
      "Epoch 123/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2572 - accuracy: 0.9644\n",
      "Epoch 124/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2468 - accuracy: 0.9616\n",
      "Epoch 125/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2391 - accuracy: 0.9672\n",
      "Epoch 126/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2372 - accuracy: 0.9644\n",
      "Epoch 127/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2401 - accuracy: 0.9625\n",
      "Epoch 128/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2383 - accuracy: 0.9644\n",
      "Epoch 129/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2178 - accuracy: 0.9719\n",
      "Epoch 130/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2048 - accuracy: 0.9803\n",
      "Epoch 131/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1936 - accuracy: 0.9803\n",
      "Epoch 132/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1836 - accuracy: 0.9841\n",
      "Epoch 133/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1826 - accuracy: 0.9822\n",
      "Epoch 134/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1667 - accuracy: 0.9831\n",
      "Epoch 135/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1554 - accuracy: 0.9878\n",
      "Epoch 136/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1516 - accuracy: 0.9878\n",
      "Epoch 137/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1445 - accuracy: 0.9888\n",
      "Epoch 138/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1393 - accuracy: 0.9888\n",
      "Epoch 139/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1319 - accuracy: 0.9906\n",
      "Epoch 140/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1262 - accuracy: 0.9944\n",
      "Epoch 141/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1250 - accuracy: 0.9934\n",
      "Epoch 142/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1285 - accuracy: 0.9906\n",
      "Epoch 143/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1232 - accuracy: 0.9934\n",
      "Epoch 144/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1141 - accuracy: 0.9925\n",
      "Epoch 145/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1065 - accuracy: 0.9953\n",
      "Epoch 146/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.1024 - accuracy: 0.9934\n",
      "Epoch 147/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.0971 - accuracy: 0.9934\n",
      "Epoch 148/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.0960 - accuracy: 0.9944\n",
      "Epoch 149/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.0957 - accuracy: 0.9944\n",
      "Epoch 150/150\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.0875 - accuracy: 0.9953\n"
     ]
    }
   ],
   "source": [
    "record = model.fit(X,y, epochs = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:54:18.856764Z",
     "iopub.status.busy": "2021-07-02T02:54:18.856602Z",
     "iopub.status.idle": "2021-07-02T02:54:18.882892Z",
     "shell.execute_reply": "2021-07-02T02:54:18.882438Z",
     "shell.execute_reply.started": "2021-07-02T02:54:18.856746Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:54:18.883723Z",
     "iopub.status.busy": "2021-07-02T02:54:18.883573Z",
     "iopub.status.idle": "2021-07-02T02:54:19.273893Z",
     "shell.execute_reply": "2021-07-02T02:54:19.273417Z",
     "shell.execute_reply.started": "2021-07-02T02:54:18.883706Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:54:19.276619Z",
     "iopub.status.busy": "2021-07-02T02:54:19.276451Z",
     "iopub.status.idle": "2021-07-02T02:54:19.381494Z",
     "shell.execute_reply": "2021-07-02T02:54:19.380994Z",
     "shell.execute_reply.started": "2021-07-02T02:54:19.276600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 1: 5.8935041427612305\n",
      "loss at epoch 150: 0.08745501190423965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f93dc716400>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"loss at epoch 1: {record.history['loss'][0]}\")\n",
    "print(f\"loss at epoch 150: {record.history['loss'][149]}\")\n",
    "plt.plot(record.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:54:19.382845Z",
     "iopub.status.busy": "2021-07-02T02:54:19.382687Z",
     "iopub.status.idle": "2021-07-02T02:54:19.385094Z",
     "shell.execute_reply": "2021-07-02T02:54:19.384648Z",
     "shell.execute_reply.started": "2021-07-02T02:54:19.382828Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_text = lines[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:54:19.385888Z",
     "iopub.status.busy": "2021-07-02T02:54:19.385742Z",
     "iopub.status.idle": "2021-07-02T02:54:19.389968Z",
     "shell.execute_reply": "2021-07-02T02:54:19.389504Z",
     "shell.execute_reply.started": "2021-07-02T02:54:19.385872Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text_seq(model,tokenizer,text_seq_length,seed_text,n_words):\n",
    "    text = []\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        encoded = pad_sequences([encoded],maxlen = text_seq_length,truncating = 'pre')\n",
    "        \n",
    "        y_predict = model.predict_classes(encoded)\n",
    "        \n",
    "        predicted_words = \" \"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == y_predict:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        seed_text = seed_text + \" \" + predicted_word\n",
    "        text.append(predicted_word)\n",
    "    return \" \".join(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T02:54:19.390740Z",
     "iopub.status.busy": "2021-07-02T02:54:19.390591Z",
     "iopub.status.idle": "2021-07-02T02:54:22.595300Z",
     "shell.execute_reply": "2021-07-02T02:54:22.594799Z",
     "shell.execute_reply.started": "2021-07-02T02:54:19.390724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-3b474758edbd>:7: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'not just inspire me but also so many others to work harder to be better and to make a dent wherever we can so very very proud to be standing alongside you so in life you know there are moments when you stop and ask yourself did i get like am i standing well this is definitely one of those moments for me and i find myself going back to the beginning back to my roots i was born to incredible parents amazing parents who served as doctors in the indian army i was the first born and as far'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_seq(model,tokenizer,50,seed_text,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
